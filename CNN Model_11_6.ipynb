{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import warnings\n",
    "warning.filterwarning(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout, Dense, Flatten, Embedding,Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D \n",
    "from keras.layers import GlobalMaxPooling1D \n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\oyo\\Desktop')\n",
    "df=pd.read_excel(r'training_29_6.xlsx',encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer('english')\n",
    "## preprocessing text:\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filter_SW = {\"very\",\"until\",\"out\",\"than\",'ain','against','aren',\"aren't\",\"arent\",'couldn',\"couldn't\",\"couldnt\",\"didn\",\"didn't\",\"didnt\",\"doesn\",\"doesn't\",\"doesnt\",\"don\",\"don't\",\"dont\",\"hadn\",\"hadn't\",\"hadnt\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"hasnt\",\"isn\",\"isn't\",\"isnt\",\"mightn't\",\"mightnt\",\"mightn\",\"mustn\",\"mustn't\",\"mustnt\",\"needn\",\"needn't\",\"neednt\",\"no\",\"not\",\"nor\",\"off\",\"shan\",\"shan't\",\"shant\",\"shouldn\",\"shouldn't\",\"shouldnt\",\"wasn't\",\"wasnt\",\"wasn\",\"weren\",\"weren't\",\"werent\",\"won't\",\"wont\",\"won\",\"wouldn\",\"wouldn't\",\"wouldnt\"}\n",
    "Nstop_words = stop_words - filter_SW\n",
    "Nstop_words.update([\"'\",\"' \",\" '\",\"’\",\" ’\",\"’ \"])\n",
    "punct = \"\"\n",
    "for i in string.punctuation:\n",
    "    if(i!=\"'\" ):\n",
    "        punct = punct+i\n",
    "#print(punct)\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def preprocessing_text(text,lemma=True):\n",
    "    text = text.lower()\n",
    "    #replacing apostrophe with none\n",
    "    text = text.replace(\"’\",\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace('/','')\n",
    "    #replacing special characters with a space\n",
    "    text = re.sub('[^A-Za-z]+', \" \", text)\n",
    "    \n",
    "    #replacing newline,tabs with none\n",
    "    text = re.sub(r\"[\\n\\t]*\", \"\", text)\n",
    "    \n",
    "    #removing multiple sapces\n",
    "    text = re.sub(\" +\",\" \", text)\n",
    "    \n",
    "    #removing punctuations except apostrophe\n",
    "    text = [word.strip(punct) for word in text.split(\" \")]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    #text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    \n",
    "    #removing stopwords\n",
    "    text = [x for x in text if x not in Nstop_words]\n",
    "    \n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    \n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    \n",
    "    # lemmatize text\n",
    "    if lemma==True:\n",
    "        text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "        \n",
    "    else:\n",
    "        ps = PorterStemmer()\n",
    "        text =  [stemmer2.stem(word) for word in text]\n",
    "        \n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=[]\n",
    "for i in range(len(df)):\n",
    "    t=preprocessing_text(str(df['Review'][i]),lemma=False)\n",
    "    stem.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= 2000)\n",
    "tokenizer.fit_on_texts(stem)\n",
    "#sequences = tokenizer.texts_to_matrix(stem)\n",
    "sequences = tokenizer.texts_to_sequences(stem)\n",
    "seq = sequence.pad_sequences(sequences , maxlen = 60,padding = 'post', truncating = 'post' )\n",
    "\n",
    "print(\"shape: \",np.shape(seq))\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = keras.layers.Reshape\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 2000, output_dim = 10 , input_length = 60))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(64,kernel_size=3, strides = 1,activation='relu', padding = 'valid'))\n",
    "model.add(MaxPooling1D(pool_size = 2, padding = 'same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel('test_18_6.xlsx')\n",
    "text=df1['Review'][:]\n",
    "'''\n",
    "model1 = SentimentIntensityAnalyzer()\n",
    "sentiment = pd.DataFrame(columns=['neg','neu','pos','compound'])\n",
    "for i in range(len(text)):\n",
    "    sentence = text[i]\n",
    "    sentence = str(sentence)\n",
    "    ss = model1.polarity_scores(sentence)\n",
    "    sentiment =  sentiment.append(ss , ignore_index = True)\n",
    "result = pd.concat([df1,sentiment], axis = 1)\n",
    "result['sentiment']=result['compound'].apply(lambda x: 0  if x<0.5 else np.nan )\n",
    "#result.head(10)\n",
    "nan_rows = result[result['sentiment'].isnull()].index.values\n",
    "result = result.drop(nan_rows)\n",
    "#result.head()\n",
    "result.to_excel('output_test1.xlsx',encoding='utf-8',index=False)\n",
    "#result.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df2 = pd.read_excel('output_test1.xlsx')\n",
    "index = []\n",
    "for i in range(len(df2['Review'])):\n",
    "    text = df2['Review'][i]\n",
    "    wordCount = len(text.split())\n",
    "    if wordCount < 3 or wordCount >60:\n",
    "        index.append(i)\n",
    "df2 = df2.drop(index)\n",
    "df2.to_excel('output_test2.xlsx',encoding = 'utf-8',index = False)\n",
    "df2.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#testr = pd.read_excel('output_test2.xlsx')\n",
    "#testing=testr\n",
    "output=pd.DataFrame(columns=['AC/Heater','Check-in Experience', 'Comfort & Safety', 'Food Experience', 'Hotel Infrastructure','Hygiene & Cleanliness','Room Equipment & Amenities','Staff & Service','TV & WiFi','Washroom'])\n",
    "result = pd.concat([df1, output], axis=1)\n",
    "result.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "com=[]\n",
    "for i in range(len(result)):\n",
    "    t=preprocessing_text(result['Review'][i])\n",
    "    com.append(t)\n",
    "seqcs = tokenizer.texts_to_sequences(com)\n",
    "seq1 = sequence.pad_sequences(seqcs , maxlen = 60)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output=pd.DataFrame(columns=['AC/Heator_Predict','Check-in Experience_Predict', 'Comfort & Safety_Predict', 'Food Experience_Predict', 'Hotel Infrastructure_Predict','Hygiene & Cleanliness_Predict','Room equipment & Amenties_Predict','Staff & Service_Predict','TV & WiFi_Predict','Washroom_Predict'])\n",
    "#final = pd.concat([result, output], axis=1)\n",
    "#final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add other classes\n",
    "add_df=df[['AC/Heater', 'Check-in Experience',\n",
    "       'Comfort & Safety', 'Food Experience', 'Hotel Infrastructure',\n",
    "       'Hygiene & Cleanliness', 'Room Equipment & Amenities', 'Staff & Service',\n",
    "       'TV & WiFi', 'Washroom']]\n",
    "#add_dF=result[['AC/Heater', 'Check-in Experience',\n",
    "#       'Comfort & Safety', 'Food Experience', 'Hotel Infrastructure',\n",
    "#       'Hygiene & Cleanliness', 'Room Equipment & Amenities', 'Staff & Service',\n",
    "#       'TV & WiFi', 'Washroom']]\n",
    "\n",
    "dict_labels={'y1_train':'AC/Heater',\n",
    "'y2_train':'Check-in Experience',\n",
    "'y3_train':'Comfort & Safety',\n",
    "'y4_train':'Food Experience',\n",
    "'y5_train':'Hotel Infrastructure',\n",
    "'y6_train':'Hygiene & Cleanliness',\n",
    "'y7_train':'Room Equipment & Amenities',\n",
    "'y8_train':'Staff & Service',\n",
    "'y9_train':'TV & WiFi',\n",
    "'y10_train':'Washroom',\n",
    "}\n",
    "'''\n",
    "dict_labels2={'y1_train':'AC/Heater',\n",
    "'y2_train':'Check-in Experience',\n",
    "'y3_train':'Comfort & Safety',\n",
    "'y4_train':'Food Experience',\n",
    "'y5_train':'Hotel Infrastructure',\n",
    "'y6_train':'Hygiene & Cleanliness',\n",
    "'y7_train':'Room Equipment & Amenities',\n",
    "'y8_train':'Staff & Service',\n",
    "'y9_train':'TV & WiFi',\n",
    "'y10_train':'Washroom',\n",
    "}'''\n",
    "\n",
    "for i in dict_labels:\n",
    "    print(\"-------\"+dict_labels[i]+\"-------\")\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(seq,df[dict_labels[i]], test_size = 0.05)\n",
    "    x_train,x_val,y_train,y_val = train_test_split(X_train,Y_train,test_size=0.08)\n",
    "    #x_test = seq1\n",
    "    #y_test = result[dict_labels[i]]\n",
    "    model.fit(x_train, y_train,batch_size = 32, validation_data=(x_val, y_val), class_weight = 'balanced', epochs=5)\n",
    "    \n",
    "    #y_score1= model.predict(x_test)\n",
    "    #y_score = clf.predict(x_test)\n",
    "    #print(y_score)\n",
    "    #for j in range(len(y_score1)):\n",
    "        #if y_score1[j] <= 0.5:\n",
    "        #    result[dict_labels[i]][j] = 0\n",
    "        #else:\n",
    "        #    result[dict_labels[i]][j] = 1\n",
    "    #result[dict_labels[i]] = y_score1\n",
    "    y_sc = model.predict(X_test)\n",
    "    y_score = np.zeros(len(y_sc))\n",
    "    for i in range(len(y_sc)):\n",
    "        if y_sc[i] < 0.51:\n",
    "            y_score[i] = 0\n",
    "        else:\n",
    "            y_score[i] = 1\n",
    "            \n",
    "    #y_score = y_sc.apply(lambda x: 0  if x<0.5 else 1 )\n",
    "    \n",
    "    print(\"The accuracy score is: {}\".format(str(accuracy_score(Y_test,y_score))))\n",
    "    print(\"The roc_auc_score is: {}\".format(str(roc_auc_score(Y_test, y_sc))))\n",
    "    print(confusion_matrix(Y_test,y_score))  \n",
    "    print(classification_report(Y_test,y_score))\n",
    "    '''\n",
    "     print(\"-------\"+dict_labels[i]+\"-------\")\n",
    "    print(accuracy_score(y_test,y_score))\n",
    "    print(confusion_matrix(y_test,y_score))  \n",
    "    print(classification_report(y_test,y_score))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
