{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk import pos_tag\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\oyo\\Desktop')\n",
    "df = pd.read_excel('training_29_6.xlsx',encoding = 'ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer('english')\n",
    "## preprocessing text:\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filter_SW = {\"very\",\"until\",\"out\",\"than\",'ain','against','aren',\"aren't\",\"arent\",'couldn',\"couldn't\",\"couldnt\",\"didn\",\"didn't\",\"didnt\",\"doesn\",\"doesn't\",\"doesnt\",\"don\",\"don't\",\"dont\",\"hadn\",\"hadn't\",\"hadnt\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"hasnt\",\"isn\",\"isn't\",\"isnt\",\"mightn't\",\"mightnt\",\"mightn\",\"mustn\",\"mustn't\",\"mustnt\",\"needn\",\"needn't\",\"neednt\",\"no\",\"not\",\"nor\",\"off\",\"shan\",\"shan't\",\"shant\",\"shouldn\",\"shouldn't\",\"shouldnt\",\"wasn't\",\"wasnt\",\"wasn\",\"weren\",\"weren't\",\"werent\",\"won't\",\"wont\",\"won\",\"wouldn\",\"wouldn't\",\"wouldnt\"}\n",
    "Nstop_words = stop_words - filter_SW\n",
    "punct = \"\"\n",
    "for i in string.punctuation:\n",
    "    if(i!=\"'\" ):\n",
    "        punct = punct+i\n",
    "#print(punct)\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def preprocessing_text(text,lemma=True):\n",
    "    text = text.lower()\n",
    "    #replacing apostrophe with none\n",
    "    text = text.replace(\"â€™\",\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace('/','')\n",
    "    #replacing special characters with a space\n",
    "    text = re.sub('[^A-Za-z]+', \" \", text)\n",
    "    \n",
    "    #replacing newline,tabs with none\n",
    "    text = re.sub(r\"[\\n\\t]*\", \"\", text)\n",
    "    \n",
    "    #removing multiple sapces\n",
    "    text = re.sub(\" +\",\" \", text)\n",
    "    \n",
    "    #removing punctuations except apostrophe\n",
    "    text = [word.strip(punct) for word in text.split(\" \")]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    #text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    \n",
    "    #removing stopwords\n",
    "    text = [x for x in text if x not in Nstop_words]\n",
    "    \n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    \n",
    "\n",
    "    \n",
    "    # lemmatize text\n",
    "    if lemma==True:\n",
    "        # pos tag text\n",
    "        pos_tags = pos_tag(text)\n",
    "        text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "        \n",
    "    else:\n",
    "        ps = PorterStemmer()\n",
    "        text =  [stemmer2.stem(word) for word in text]\n",
    "        \n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem1 = []\n",
    "for i in range(len(df)):\n",
    "    text = str(df['Review'][i])\n",
    "    t = preprocessing_text(text, lemma = False)\n",
    "    stem1.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_Vector = TfidfVectorizer(max_features=2000, min_df=0.001, max_df=0.5,stop_words=Nstop_words,ngram_range = (1,4))\n",
    "x = t_Vector.fit_transform(stem1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape\n",
    "#t_Vector.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding L2 tags\n",
    "\n",
    "df_cols = df[['L2 AC/Heater', 'L2 Check-in Experience',\n",
    "       'L2 Comfort & Safety', 'L2 Food Experience', 'L2 Hotel Infrastructure',\n",
    "       'L2 Hygiene & Cleanliness', 'L2 Room Equipment & Amenities', 'L2 Staff & Service',\n",
    "       'L2 TV & WiFi', 'L2 Washroom']]\n",
    "dict_labels = {'y1':'L2 AC/Heater',\n",
    "'y2':'L2 Check-in Experience',\n",
    "'y3':'L2 Comfort & Safety',\n",
    "'y4':'L2 Food Experience',\n",
    "'y5':'L2 Hotel Infrastructure',\n",
    "'y6':'L2 Hygiene & Cleanliness',\n",
    "'y7':'L2 Room Equipment & Amenities',\n",
    "'y8':'L2 Staff & Service',\n",
    "'y9':'L2 TV & WiFi',\n",
    "'y10':'L2 Washroom',\n",
    "}\n",
    "for i in dict_labels:\n",
    "    text = df_cols[dict_labels[i]]\n",
    "    encode = LabelEncoder()\n",
    "    df[dict_labels[i]] = encode.fit_transform(text.astype(str))        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_labels_L1 = {'y1':'AC/Heater',\n",
    "'y2':'Check-in Experience',\n",
    "'y3':'Comfort & Safety',\n",
    "'y4':'Food Experience',\n",
    "'y5':'Hotel Infrastructure',\n",
    "'y6':'Hygiene & Cleanliness',\n",
    "'y7':'Room Equipment & Amenities',\n",
    "'y8':'Staff & Service',\n",
    "'y9':'TV & WiFi',\n",
    "'y10':'Washroom',\n",
    "}\n",
    "dict_labels_L2 = {'y1':'L2 AC/Heater',\n",
    "'y2':'L2 Check-in Experience',\n",
    "'y3':'L2 Comfort & Safety',\n",
    "'y4':'L2 Food Experience',\n",
    "'y5':'L2 Hotel Infrastructure',\n",
    "'y6':'L2 Hygiene & Cleanliness',\n",
    "'y7':'L2 Room Equipment & Amenities',\n",
    "'y8':'L2 Staff & Service',\n",
    "'y9':'L2 TV & WiFi',\n",
    "'y10':'L2 Washroom',\n",
    "}\n",
    "for i in dict_labels_L1:\n",
    "    print(\"-------\"+dict_labels_L1[i]+\"-------\")\n",
    "    df1  = pd.DataFrame()\n",
    "    df1 = df1.append(df[df[dict_labels_L1[i]] == 1])\n",
    "    #DF1 = DF1.append(final[final[dict_labels_L1[i]] == 1])\n",
    "    df1 = df1.reset_index()\n",
    "    #DF1 = DF1.reset_index()\n",
    "    #print(len(DF1))\n",
    "    y = df1[dict_labels_L2[i]]\n",
    "    #y1 = DF1[dict_labels_L2[i]]\n",
    "    #y = y.replace(np.nan, 0)\n",
    "    #y = y.fillna(str(0))\n",
    "    #X,Y = nr.fit_sample(df1['Review'],y)\n",
    "    stem=[]\n",
    "    #stem1 = []\n",
    "    for j in range(len(df1)):\n",
    "        #t = str(df1['Review'][j])\n",
    "        t=preprocessing_text(str(df1['Review'][j]),lemma=False)\n",
    "        stem.append(t)\n",
    "    \n",
    "    x = t_Vector.transform(stem)\n",
    "    x_train,x_test,y_train,Y_test = train_test_split(x,y,test_size=0.15,random_state = 20)\n",
    "    model = RFE(RandomForestClassifier(max_features = 'auto', criterion = 'gini',bootstrap = True,class_weight = 'balanced'), 100, step=1)\n",
    "    #model = SVC(kernel = 'rbf',class_weight = 'balanced', decision_function_shape = 'ovr', C = 1000000,gamma = 1e-06 )\n",
    "    model.fit(x_train,y_train)\n",
    "    y_score = model.predict(x_test)\n",
    "    y_true = model.predict(x_train)\n",
    "    \n",
    "    print(accuracy_score(Y_test,y_score))\n",
    "    print(confusion_matrix(Y_test,y_score))\n",
    "    \n",
    "    print(accuracy_score(y_train,y_true))\n",
    "    print(confusion_matrix(y_train,y_true))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
